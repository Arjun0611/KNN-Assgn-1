{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4059e71-38ff-4aec-946a-08a37c6d36a9",
   "metadata": {},
   "source": [
    "# KNN Assignment - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a445c281-3f83-48cc-91ab-0e4c1e4b2076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. \n",
    "# It classifies or predicts a data point's label based on the majority class or average value of its k-nearest neighbors in the feature space. It is applicable to both classification and regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9a374f5-c94c-419d-98b8-d66d5cac432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.\n",
    "\n",
    "# Odd K for Binary Classification: Use odd K values for binary classification to avoid ties.\n",
    "# Cross-validation: Experiment with different values of K and use cross-validation to assess performance.\n",
    "# Rule of thumb: Square root of the number of data points is a common starting point.\n",
    "# Consider Dataset Characteristics: Choose K based on the dataset's noise level and complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86d0589e-6a94-49ae-829e-0ee82665581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.\n",
    "\n",
    "# KNN Classifier\n",
    "#1. Used for classification problems.\n",
    "#2. Predicts the class label of a new data point.\n",
    "#3. Assigns the class label based on the majority class of its K-nearest neighbors.\n",
    "\n",
    "# KNN Regressor\n",
    "#1. Used for regression problems.\n",
    "#2. Predicts a continuous value for a new data point.\n",
    "#3. Predicts the average or weighted average of the target values of its K-nearesr neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a5e6191-dd79-447d-bd81-016d29bd7fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4.\n",
    "\n",
    "# Performance Measurement of KNN:\n",
    "\n",
    "# Classification\n",
    "#1. Accuracy: Ratio of correctly predicted instances to the total instances.\n",
    "#2. Precision, Recall, F1-Score: For a detailed understanding of class-wise performance.\n",
    "#3. Confusion Matrix: Provides a summary of correct and incorrect predictions.\n",
    "\n",
    "# Regression\n",
    "#1. MSE: Measures the average of the squared differences b/w predicted and observed values.\n",
    "#2. R2-Score: Indicates the proportion of the variance in the dependent variable that is predictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f173380f-85e6-4ec6-b758-08e368af9714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5.\n",
    "\n",
    "# As the number of dimensions/features increases, the distance between points becomes more uniform, impacting the effectiveness of KNN.\n",
    "# In high-dimensional spaces, most data points are at similar distances from each other, reducing the discriminatory power of proximity-based algorithms like KNN.\n",
    "# Computationally expensive as distances need to be calculated in each dimension, leading to increased time complexity.\n",
    "# Diminishing returns on increased dimensionality for improving classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66219fb0-2347-4779-88bb-228c3914cd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6.\n",
    "\n",
    "# In KNN, missing values can be handled by imputing them based on the mean, median, or mode of the available neighbors in the feature space.\n",
    "# The algorithm calculates distances and identifies nearest neighbors, and the missing values are imputed by aggregating values from those neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d340b4fc-b6ff-419a-a118-524fec909d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7.\n",
    "\n",
    "# The KNN classifier is suitable for classification problems where the o/p is a categorical label, while the KNN regressor is appropriate for regression problems where the o/p is a continuous numerical value.\n",
    "# The choice depends on the nature of the problem - use the classifier for classification tasks and the regressor for regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7847ed9-fdea-479c-9a78-76d2595ef062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8.\n",
    "\n",
    "# Strengths\n",
    "# Simple and easy to implement.\n",
    "# No training phase, making it effective for quick predictions.\n",
    "# Works well for small to medium-sized datasets.\n",
    "\n",
    "# Weaknesses\n",
    "# Computationally expensive during prediction for larger datasets.\n",
    "# Sensitive to irrelevant and redundant features.\n",
    "# Requires careful preprocessing, especially with missing data.\n",
    "\n",
    "# Addressing Weaknesses\n",
    "# Feature selection or dimensionality reduction.\n",
    "# Data normalization for feature scaling.\n",
    "# Optimize parameters such as the number of neighbors.\n",
    "# Impute missing values or consider weighted distances.\n",
    "\n",
    "# Classification specific:\n",
    "# May struggle with imbalanced datasets; use techniques like oversampling or undersampling.\n",
    "# Select appropriate distance metrics based on data charactersitics.\n",
    "\n",
    "# Regression-Specific:\n",
    "# Consider local weightings for better regression results.\n",
    "# Evaluate performance using suitable regression metrics like MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8f9bf70-a539-4ab5-af35-bdd89cee3c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9.\n",
    "\n",
    "# Euclidean Distance:\n",
    "# Measures the straight-line distance between two points.\n",
    "# Given by the sqrt of the sum of squared differences in coordinates.\n",
    "# Sensitive to magnitude differences between features.\n",
    "\n",
    "# Manhattan Distance:\n",
    "# Measures the sum of absolute differences along each dimesnion.\n",
    "# Less sensitive to magnitude differences, making it robust.\n",
    "\n",
    "# Differences:\n",
    "\n",
    "# Euclidean is sensitive to magnitude, while Manhattan is not.\n",
    "# Euclidean is the diagonal distance, Manhattan is the sum of horizontal and vertical distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5c786bd-4737-4267-8885-47893bce04be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10.\n",
    "\n",
    "# Uniform Scale: KNN relies on distances between data points.\n",
    "# Feature Magnitudes: Features with larger magnitudes dominate the distance calculation.\n",
    "# Equal Importance: Scaling ensures all features contribute equally to distance measures.\n",
    "# Improves Accuracy: Prevents bias towards features with larger scales.\n",
    "# Standardization or Normalization: Common scaling methods for KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16d75a3-ab45-4389-8b01-1f06254eca35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
